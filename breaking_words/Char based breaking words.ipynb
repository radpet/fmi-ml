{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, path):\n",
    "    request = requests.get(url)\n",
    "    with ZipFile(BytesIO(request.content), \"r\") as file:\n",
    "         file.extractall(path)\n",
    "\n",
    "urls = [\n",
    "    \"https://chitanka.info/book/25-andersenovi-prikazki.txt.zip\",\n",
    "    \"https://chitanka.info/book/3103-601-izpitani-gotvarski-retsepti.txt.zip\",\n",
    "    \"https://chitanka.info/book/6393-bogat-tatko-beden-tatko.txt.zip\",\n",
    "    \"https://chitanka.info/book/524-az-i-moreto.txt.zip\",\n",
    "    \"https://chitanka.info/book/6548-baba-djado-i-vnuche.txt.zip\",\n",
    "    \"https://chitanka.info/book/6938-12-printsipa-na-proizvoditelnostta.txt.zip\"\n",
    "    \n",
    "]\n",
    "            \n",
    "for url in urls:\n",
    "    download_and_extract(url,\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"data/Hans_Kristian_Andersen_-_Andersenovi_prikazki_-25-b.txt\",\n",
    "    \"data/Penka_Cholcheva_-_601_izpitani_gotvarski_retsepti_-3103-b.txt\",\n",
    "    \"data/Robyrt_Kijosaki_-_Bogat_tatko_beden_tatko_-_Na_kakvo_bogatite_uchat_detsata_si_za_parite_a_bednite_i_srednata_klasa_-_ne-6393-b.txt\",\n",
    "    \"data/Petja_Dubarova_-_Az_i_moreto_-524-b.txt\",\n",
    "    \"data/Baba_djado_i_vnuche_-_Narodni_prikazki-6548-b.txt\",\n",
    "    \"data/Haringtyn_Emersyn_-_12_printsipa_na_proizvoditelnostta_-6938-b.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(files):\n",
    "    lines = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, encoding=\"utf-8-sig\") as file_content:\n",
    "            for line in file_content:\n",
    "                line = line.strip()\n",
    "                if(len(line) > max_line_len):\n",
    "                    tokens = line.split(\" \")\n",
    "                    new_line = \"\"\n",
    "                    for token in tokens:\n",
    "                        if(len(new_line) + len(token) < max_line_len):\n",
    "                            new_line +=token + \" \"\n",
    "                        else:\n",
    "                            lines.append(new_line)\n",
    "                            new_line = \"\"\n",
    "                else:\n",
    "                    if len(line) > 3:\n",
    "                        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_corpus(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Партиздат, София, 1973',\n",
       " 'Редактор: Борис Въжаров',\n",
       " 'Коректор: Веселина Цветкова']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [ line.replace(\" \", \"\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_CHAR = \"\\t\"\n",
    "END_CHAR = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = [ START_CHAR + line + END_CHAR for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ХансКристианАндерсен', 'Андерсеновиприказки', 'Огнивото']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch = set()\n",
    "for line in input_text:\n",
    "    for c in line:\n",
    "        input_ch.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ch = list(sorted(input_ch))\n",
    "input_ch_len = len(input_ch)\n",
    "input_ch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = set()\n",
    "for line in target_text:\n",
    "    for c in line:\n",
    "        target_ch.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ch_len = len(target_ch)\n",
    "target_ch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch_idx = dict([(char,i) for i, char in enumerate(input_ch)])\n",
    "target_ch_idx = dict([(char,i) for i, char in enumerate(target_ch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "max_input_len = max([len(line) for line in input_text])\n",
    "max_target_len = max([len(line) for line in target_text])\n",
    "\n",
    "print(max_input_len)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros ( (len(input_text), max_input_len, input_ch_len ),dtype=\"float64\" )\n",
    "decoder_input_data = np.zeros ( (len(input_text), max_target_len, target_ch_len ),dtype=\"float64\" )\n",
    "decoder_target_data = np.zeros( (len(input_text), max_target_len, target_ch_len ),dtype=\"float64\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32262, 46, 148)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32262, 52, 151)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32262, 52, 151)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_name, target_name) in enumerate(zip(input_text, target_text)):\n",
    "    for t, char in enumerate(input_name):\n",
    "        encoder_input_data[i, t, input_ch_idx[char]] = 1.\n",
    "    for t, char in enumerate(target_name):\n",
    "        decoder_input_data[i, t, target_ch_idx[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_ch_idx[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, input_ch_len))\n",
    "encoder = LSTM(STATE_SPACE_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, target_ch_len))\n",
    "dropout = Dropout(0.2)(decoder_inputs)\n",
    "decoder_lstm = LSTM(STATE_SPACE_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dropout,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(target_ch_len, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 151)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None, 148)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 151)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 414720      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  417792      dropout_1[0][0]                  \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 151)    38807       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 871,319\n",
      "Trainable params: 871,319\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25809 samples, validate on 6453 samples\n",
      "Epoch 1/256\n",
      "25809/25809 [==============================] - 20s 772us/step - loss: 2.7717 - val_loss: 2.4526\n",
      "Epoch 2/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 2.3762 - val_loss: 2.2507\n",
      "Epoch 3/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 2.2596 - val_loss: 2.1676\n",
      "Epoch 4/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 2.1868 - val_loss: 2.1000\n",
      "Epoch 5/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 2.1126 - val_loss: 2.0359\n",
      "Epoch 6/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 2.0525 - val_loss: 1.9806\n",
      "Epoch 7/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 1.9892 - val_loss: 1.9062\n",
      "Epoch 8/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.9327 - val_loss: 1.8618\n",
      "Epoch 9/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.8816 - val_loss: 1.8078\n",
      "Epoch 10/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 1.8309 - val_loss: 1.7517\n",
      "Epoch 11/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 1.7789 - val_loss: 1.7025\n",
      "Epoch 12/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.7233 - val_loss: 1.6489\n",
      "Epoch 13/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.6686 - val_loss: 1.5876\n",
      "Epoch 14/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.6174 - val_loss: 1.5500\n",
      "Epoch 15/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 1.5673 - val_loss: 1.4808\n",
      "Epoch 16/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.5185 - val_loss: 1.4396\n",
      "Epoch 17/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 1.4706 - val_loss: 1.4009\n",
      "Epoch 18/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 1.4238 - val_loss: 1.3562\n",
      "Epoch 19/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.3759 - val_loss: 1.2963\n",
      "Epoch 20/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.3321 - val_loss: 1.2527\n",
      "Epoch 21/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.2859 - val_loss: 1.1959\n",
      "Epoch 22/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.2409 - val_loss: 1.1581\n",
      "Epoch 23/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 1.1988 - val_loss: 1.1143\n",
      "Epoch 24/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 1.1638 - val_loss: 1.0830\n",
      "Epoch 25/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 1.1298 - val_loss: 1.0417\n",
      "Epoch 26/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.0957 - val_loss: 1.0067\n",
      "Epoch 27/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.0660 - val_loss: 0.9904\n",
      "Epoch 28/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 1.0378 - val_loss: 0.9632\n",
      "Epoch 29/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 1.0100 - val_loss: 0.9473\n",
      "Epoch 30/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.9846 - val_loss: 0.9111\n",
      "Epoch 31/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.9612 - val_loss: 0.9031\n",
      "Epoch 32/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.9378 - val_loss: 0.8809\n",
      "Epoch 33/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.9162 - val_loss: 0.8453\n",
      "Epoch 34/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.8910 - val_loss: 0.8489\n",
      "Epoch 35/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.8723 - val_loss: 0.8256\n",
      "Epoch 36/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.8501 - val_loss: 0.8062\n",
      "Epoch 37/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.8283 - val_loss: 0.7695\n",
      "Epoch 38/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.8130 - val_loss: 0.7675\n",
      "Epoch 39/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.7943 - val_loss: 0.7538\n",
      "Epoch 40/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.7809 - val_loss: 0.7371\n",
      "Epoch 41/256\n",
      "25809/25809 [==============================] - 19s 740us/step - loss: 0.7599 - val_loss: 0.7268\n",
      "Epoch 42/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.7447 - val_loss: 0.7078\n",
      "Epoch 43/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.7328 - val_loss: 0.6937\n",
      "Epoch 44/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.7161 - val_loss: 0.6823\n",
      "Epoch 45/256\n",
      "25809/25809 [==============================] - 19s 742us/step - loss: 0.7022 - val_loss: 0.6678\n",
      "Epoch 46/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.6858 - val_loss: 0.6621\n",
      "Epoch 47/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.6752 - val_loss: 0.6479\n",
      "Epoch 48/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.6670 - val_loss: 0.6322\n",
      "Epoch 49/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.6516 - val_loss: 0.6221\n",
      "Epoch 50/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 0.6410 - val_loss: 0.6174\n",
      "Epoch 51/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.6296 - val_loss: 0.6111\n",
      "Epoch 52/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 0.6210 - val_loss: 0.6091\n",
      "Epoch 53/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.6100 - val_loss: 0.6192\n",
      "Epoch 54/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 0.6006 - val_loss: 0.5821\n",
      "Epoch 55/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.5900 - val_loss: 0.5759\n",
      "Epoch 56/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.5838 - val_loss: 0.5695\n",
      "Epoch 57/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.5756 - val_loss: 0.5696\n",
      "Epoch 58/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.5650 - val_loss: 0.5440\n",
      "Epoch 59/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.5572 - val_loss: 0.5597\n",
      "Epoch 60/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.5521 - val_loss: 0.5481\n",
      "Epoch 61/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.5443 - val_loss: 0.5540\n",
      "Epoch 62/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.5327 - val_loss: 0.5405\n",
      "Epoch 63/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.5298 - val_loss: 0.5210\n",
      "Epoch 64/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.5235 - val_loss: 0.5202\n",
      "Epoch 65/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.5171 - val_loss: 0.5218\n",
      "Epoch 66/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.5081 - val_loss: 0.5328\n",
      "Epoch 67/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.5029 - val_loss: 0.5148\n",
      "Epoch 68/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.4947 - val_loss: 0.5095\n",
      "Epoch 69/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.4888 - val_loss: 0.4997\n",
      "Epoch 70/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.4845 - val_loss: 0.5081\n",
      "Epoch 71/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.4798 - val_loss: 0.4921\n",
      "Epoch 72/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 0.4726 - val_loss: 0.4883\n",
      "Epoch 73/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.4647 - val_loss: 0.4757\n",
      "Epoch 74/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.4606 - val_loss: 0.4804\n",
      "Epoch 75/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.4582 - val_loss: 0.4742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.4516 - val_loss: 0.4712\n",
      "Epoch 77/256\n",
      "25809/25809 [==============================] - 19s 740us/step - loss: 0.4446 - val_loss: 0.4776\n",
      "Epoch 78/256\n",
      "25809/25809 [==============================] - 19s 735us/step - loss: 0.4397 - val_loss: 0.4617\n",
      "Epoch 79/256\n",
      "25809/25809 [==============================] - 19s 736us/step - loss: 0.4347 - val_loss: 0.4568\n",
      "Epoch 80/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.4300 - val_loss: 0.4622\n",
      "Epoch 81/256\n",
      "25809/25809 [==============================] - 19s 740us/step - loss: 0.4260 - val_loss: 0.4604\n",
      "Epoch 82/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.4239 - val_loss: 0.4547\n",
      "Epoch 83/256\n",
      "25809/25809 [==============================] - 19s 739us/step - loss: 0.4150 - val_loss: 0.4464\n",
      "Epoch 84/256\n",
      "25809/25809 [==============================] - 19s 737us/step - loss: 0.4109 - val_loss: 0.4471\n",
      "Epoch 85/256\n",
      "25809/25809 [==============================] - 19s 738us/step - loss: 0.4086 - val_loss: 0.4495\n",
      "Epoch 86/256\n",
      " 9088/25809 [=========>....................] - ETA: 11s - loss: 0.4054"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-51332f075ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=256,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save(\"char_based_rnn_baseline_86_epochs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(STATE_SPACE_DIM,))\n",
    "decoder_state_input_c = Input(shape=(STATE_SPACE_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_ch_idx = dict(\n",
    "    (i, char) for char, i in input_ch_idx.items())\n",
    "reverse_target_ch_idx= dict(\n",
    "    (i, char) for char, i in target_ch_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  \n",
    "    target_seq = np.zeros((1, 1, len(target_ch)))\n",
    "   \n",
    "    target_seq[0, 0, target_ch_idx['$']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_ch_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        \n",
    "        if (sampled_char == END_CHAR or\n",
    "           len(decoded_sentence) > max_target_len):\n",
    "            stop_condition = True\n",
    "\n",
    "       \n",
    "        target_seq = np.zeros((1, 1, len(target_ch)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ХансКристианАндерсен\n",
      "Decoded sentence: Хан „Растици най-дебер\n",
      "\n",
      "-\n",
      "Input sentence: Андерсеновиприказки\n",
      "Decoded sentence: Ан ден себя по при как из\n",
      "\n",
      "-\n",
      "Input sentence: Огнивото\n",
      "Decoded sentence: Огиновото\n",
      "\n",
      "-\n",
      "Input sentence: Изширокияпътвървешевойник:едно,две,едно,\n",
      "Decoded sentence: Из изкили то пръветвеше болни. Ще доде, дедна, \n",
      "\n",
      "-\n",
      "Input sentence: Нагърбасиносешераница,анакръста—сабя,\n",
      "Decoded sentence: На гъбра си носе зернаци, а на най траса; — бабя \n",
      "\n",
      "-\n",
      "Input sentence: бешеходилнавойнаисегасевръщашеудомаси.\n",
      "Decoded sentence: беше ходила в някона се изсегат бъдеше магьоса и \n",
      "\n",
      "-\n",
      "Input sentence: пътягосрещнаеднастарамагьосница.Тябеше\n",
      "Decoded sentence: път ягосреща неднастара магьосница. Тя беше \n",
      "\n",
      "-\n",
      "Input sentence: —Добървечер,войниче!—казамагьосницата.—\n",
      "Decoded sentence: — Добър вечер, в гоне! — чака за могьосанцита. — \n",
      "\n",
      "-\n",
      "Input sentence: хубавасабяикакваголямараницаимаш!Тиси\n",
      "Decoded sentence: хубава са бията когава пляма на имаши. — изче \n",
      "\n",
      "-\n",
      "Input sentence: —Благодаряти,старамагьоснице!—отвърна\n",
      "Decoded sentence: — Благодарят, стара само знусице, — отвърна \n",
      "\n",
      "-\n",
      "Input sentence: —Виждашлитоваголямодърво?—рече\n",
      "Decoded sentence: — Виждаш ли това голямо дърво? — рече \n",
      "\n",
      "-\n",
      "Input sentence: ипосочиеднодърво,коеторастешенаблизо.—То\n",
      "Decoded sentence: и посочи едно дърво, което растеше на биро. — То \n",
      "\n",
      "-\n",
      "Input sentence: съвсемкухоотвътре.Акосеизкачишнавърхаму,\n",
      "Decoded sentence: със смешко от въртент. Доми се казиша в хардука \n",
      "\n",
      "-\n",
      "Input sentence: видишеднадупка,покоятоможешдасеспуснеш\n",
      "Decoded sentence: ви изиден да пупка, от отголо му да се сърсушен \n",
      "\n",
      "-\n",
      "Input sentence: досамитекорени.Азщетивържаедновъжеза\n",
      "Decoded sentence: до самите корени. Аз ще ти въждален добуда \n",
      "\n",
      "-\n",
      "Input sentence: —Каквощеправявдървото?—попитавойникът.\n",
      "Decoded sentence: — Какво ще правя в дървото? — попита войникът. \n",
      "\n",
      "-\n",
      "Input sentence: —Щесивземешпари!—казамагьосницата.—\n",
      "Decoded sentence: — Ще си вземеш рази. Така за могьосницата. — \n",
      "\n",
      "-\n",
      "Input sentence: щомсеспуснешдокорените,щесенамеришведна\n",
      "Decoded sentence: щом се спуснеш до понужите, ще се намеше рева в \n",
      "\n",
      "-\n",
      "Input sentence: стая;тамемногосветло,защотогорятповечеот\n",
      "Decoded sentence: стая; там ме го но сволен, защото готяр от чевето \n",
      "\n",
      "-\n",
      "Input sentence: лампи.Предсебесищевидиштриврати.Тище\n",
      "Decoded sentence: лампи. Пред себе си ще видиш и тривара. Ти ще \n",
      "\n",
      "-\n",
      "Input sentence: дагиотвориш,защотоключоветесанадтях.\n",
      "Decoded sentence: да ги отвориш, защото ключвето чесята за те. \n",
      "\n",
      "-\n",
      "Input sentence: влезешвпърватастая,щевидишнасредподаголям\n",
      "Decoded sentence: вреше в 2 ръвта тастя, ще видиш на сред да поглави \n",
      "\n",
      "-\n",
      "Input sentence: върхукойтоседиеднокуче.Очитемусаголеми\n",
      "Decoded sentence: върху който седедни ножчукте. Не ти само дам си \n",
      "\n",
      "-\n",
      "Input sentence: чаеничаши,нотинямадасеплашишоттова.Аз\n",
      "Decoded sentence: чаени чаши, нито няма на се плушият кота. Ах \n",
      "\n",
      "-\n",
      "Input sentence: тидаммоятасиняпрестилканаквадратчета,зада\n",
      "Decoded sentence: ти дама моят си напрестилна каква да тредата, а не \n",
      "\n",
      "-\n",
      "Input sentence: постелешнапода,послещеграбнешкучето,щего\n",
      "Decoded sentence: по стееше налгода. Посегне брадкеше кучето, бяд \n",
      "\n",
      "-\n",
      "Input sentence: върхупрестилката,щеотворишсандъкаище\n",
      "Decoded sentence: върху престилата, ще от върхи с надък ще и а \n",
      "\n",
      "-\n",
      "Input sentence: колкотопариискаш.Тамсамеднитепари.Ако\n",
      "Decoded sentence: колкото пари си каш. Там самедените пра. Ако \n",
      "\n",
      "-\n",
      "Input sentence: чистосребро,щеотидешвъввторатастая;там\n",
      "Decoded sentence: чисто сребро, ще отидеш в въвтората; стаята и \n",
      "\n",
      "-\n",
      "Input sentence: сандъкаседи,другокучесочиколкотоводенични\n",
      "Decoded sentence: са да късне ид, друго чу се учи колкото водочиние \n",
      "\n",
      "-\n",
      "Input sentence: типакнесеплаши,турнигосамовърху\n",
      "Decoded sentence: ти пак не се плаши. Обули само добрука \n",
      "\n",
      "-\n",
      "Input sentence: миисивземипари.Аколипъкпожелаешзлато—и\n",
      "Decoded sentence: ми и си вземи пари. Ако ли пълко пределаха тилки \n",
      "\n",
      "-\n",
      "Input sentence: щеимаш,колкотоможешданосиш,стигасамода\n",
      "Decoded sentence: ще имаш, колкото можеш да нисъни, с касваме го \n",
      "\n",
      "-\n",
      "Input sentence: втретатастая.Наистинакучето,коетостоивърху\n",
      "Decoded sentence: в третата стая. Наистина учуто, което от съпвите да \n",
      "\n",
      "-\n",
      "Input sentence: съсзлатото,имаочи,големиколкотокръгликули.\n",
      "Decoded sentence: със златото, има човеко или мелкото кокролки киле, \n",
      "\n",
      "-\n",
      "Input sentence: естрашнокуче!Нотииотнегонямадасе\n",
      "Decoded sentence: е страшно кучен Но и ти и отнего на заеда с \n",
      "\n",
      "-\n",
      "Input sentence: Турнигонапрестилкатами,тонещетинаправи\n",
      "Decoded sentence: Турнига по срел т кафията, което не те заправи \n",
      "\n",
      "-\n",
      "Input sentence: —Тованеелошо!—речевойникът.—Нокаквоще\n",
      "Decoded sentence: — Това не е лио! Те бели съдетно. — Тока той дава \n",
      "\n",
      "-\n",
      "Input sentence: датидамаз,старамагьоснице?Разбирасе,ти\n",
      "Decoded sentence: да ти замас, а рама слано и шер. — Разбирате си \n",
      "\n",
      "-\n",
      "Input sentence: —Не—казамагьосницата,—неискамнитопетак.\n",
      "Decoded sentence: — Не — каза магьосницата, не — симак не от типат, \n",
      "\n",
      "-\n",
      "Input sentence: трябвадамидонесешсамостаротоогниво,което\n",
      "Decoded sentence: трябва за ми едно се смащни по той отговино, които \n",
      "\n",
      "-\n",
      "Input sentence: бабазабрави,когатобешезапоследенпъттам\n",
      "Decoded sentence: баба забрави, когато беше запосле дъретнат към \n",
      "\n",
      "-\n",
      "Input sentence: —Добре!Вържимесвъжето!—казавойникът.\n",
      "Decoded sentence: — Добре! Вържише съвежноте! — каза от шинок. \n",
      "\n",
      "-\n",
      "Input sentence: —Сега!—извикамагьосницата.—Етотиимоята\n",
      "Decoded sentence: — Сега! — извика магьосницата. — Ето и ти тоя ба \n",
      "\n",
      "-\n",
      "Input sentence: престилканаквадратчета.Тогававойникътсе\n",
      "Decoded sentence: престилка на квадратате. Тогава войнат сък и \n",
      "\n",
      "-\n",
      "Input sentence: надървото,спуснасевдупкатаисенамеридолу,\n",
      "Decoded sentence: на дървото, спусна се в дупката ни сема е люздено, \n",
      "\n",
      "-\n",
      "Input sentence: голяматастая,къдетогоряха,кактобешеказала\n",
      "Decoded sentence: голямата стая, чъд от обял зато, как ще бишаха па \n",
      "\n",
      "-\n",
      "Input sentence: Тойотворипърватаврата.Тамседешекучетосочи\n",
      "Decoded sentence: То тов от привърната рата. Та се заеше чът соките \n",
      "\n",
      "-\n",
      "Input sentence: —Здравей,приятелю!—казавойникът,сложи\n",
      "Decoded sentence: — Зарядва. Преял тих: „Каква я осънки? Къдали \n",
      "\n",
      "-\n",
      "Input sentence: върхупрестилкатанамагьосницатаинатъпка\n",
      "Decoded sentence: върху престилата на масьо ни катината лъка с \n",
      "\n",
      "-\n",
      "Input sentence: сисмеднипари.Следтоватойзатворисандъка,\n",
      "Decoded sentence: си смедни пари. След товато й бялота в суздана, \n",
      "\n",
      "-\n",
      "Input sentence: кучетопакнастаротомумястоивлезевъв\n",
      "Decoded sentence: чукето пак настарото му мисто и вледверв \n",
      "\n",
      "-\n",
      "Input sentence: стая.Ах!Тукнасандъкаседешекучето,което\n",
      "Decoded sentence: стая. Ах Ам тукнаса за не бъдеше чучкоте, което \n",
      "\n",
      "-\n",
      "Input sentence: —Нямазащотакадасеблещишсрещумен!—рече\n",
      "Decoded sentence: — Няма защото шага седребиш с мъдеше! — рече не \n",
      "\n",
      "-\n",
      "Input sentence: —Щетезаболяточите.Итойсложикучетовърху\n",
      "Decoded sentence: — Ще те заболят отиче. И той служи кучетъх върха \n",
      "\n",
      "-\n",
      "Input sentence: намагьосницата.Нокогатовидясребърнитепарив\n",
      "Decoded sentence: на магьосницата. Но когато видя сърберните прави \n",
      "\n",
      "-\n",
      "Input sentence: изхвърливсичкимеднипариинапълниджобоветеи\n",
      "Decoded sentence: изхвърли всички медни пари на лъпно държивоте и \n",
      "\n",
      "-\n",
      "Input sentence: сисамосъссребро.Сетнеотидевтретатастая.\n",
      "Decoded sentence: си само със сребро. Сетне от едите вратата. Тя се \n",
      "\n",
      "-\n",
      "Input sentence: тукнаистинабешестрашно!Очитенакучето,което\n",
      "Decoded sentence: тук наистина беше стран шено, те качице, което бих \n",
      "\n",
      "-\n",
      "Input sentence: вътре,бяхаголемиколкотокулиисевъртяхав\n",
      "Decoded sentence: вътре, бяха големи колкото куличи се връхвата \n",
      "\n",
      "-\n",
      "Input sentence: —Добървечер!—казавойникътиотдадечест,\n",
      "Decoded sentence: — До бърве беш! Тяка за войникът отода двете, с \n",
      "\n",
      "-\n",
      "Input sentence: никогапрезживотасинебешевиждалтаковакуче.\n",
      "Decoded sentence: никога през живота си не беше виждалата къдката, \n",
      "\n",
      "-\n",
      "Input sentence: постоямалко,тойграбнакучето,сложиговърху\n",
      "Decoded sentence: постият млагото й слада букнете, поковил държа о \n",
      "\n",
      "-\n",
      "Input sentence: иотворисандъка.Господи,колкомногозлато\n",
      "Decoded sentence: и от вирна седна. Кога с голи, колко многа лово \n",
      "\n",
      "-\n",
      "Input sentence: втоясандък!Стияпаривойникътможешедакупи\n",
      "Decoded sentence: в тоя санъч, „то ти пари в нокошкоте редуша му и \n",
      "\n",
      "-\n",
      "Input sentence: столиченград,всичкизахарнипрасенцаот\n",
      "Decoded sentence: стогилен град, всички захарни преснаха тов \n",
      "\n",
      "-\n",
      "Input sentence: всичкиоловнивойници,камшициидървеникончета\n",
      "Decoded sentence: всички оловни в ношия каши и цим движеш стонате на \n",
      "\n",
      "-\n",
      "Input sentence: света.Да,туйсеказвашебогатство!Ивойникът\n",
      "Decoded sentence: света. Да, те скай баша везостатовст, бялковето \n",
      "\n",
      "-\n",
      "Input sentence: всичкисребърнипариотджобоветеираницатасии\n",
      "Decoded sentence: всички сребрен при на поводе от цеберата и сицита \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: тяхсивзезлатни.Тойнатъпкасжълтицинесамо\n",
      "Decoded sentence: тях си взезлят на. Той не так пък в пениците са \n",
      "\n",
      "-\n",
      "Input sentence: ираницата,ноифуражкатаичизмитеси,тъйче\n",
      "Decoded sentence: и раницата, но и упраканата и черимите, стъче, \n",
      "\n",
      "-\n",
      "Input sentence: можешедаседвижи.Сегатойнаистинаимашемного\n",
      "Decoded sentence: можеше да седвижи. Саготена соти и самие на можа \n",
      "\n",
      "-\n",
      "Input sentence: Сложипаккучетовърхусандъка,затвориврататаи\n",
      "Decoded sentence: Сложи пак кучето върху саднок, за тарвори в тарика \n",
      "\n",
      "-\n",
      "Input sentence: —Дърпайнагоре,старамагьоснице!\n",
      "Decoded sentence: — Дърра сален, преста за магьоснице!\n",
      "\n",
      "-\n",
      "Input sentence: —Взелиогнивото?—попитамагьосницата.\n",
      "Decoded sentence: — Взели ги новито? — попита магьосницата. \n",
      "\n",
      "-\n",
      "Input sentence: —Подяволите!—извикавойникът.—Негосъвсем\n",
      "Decoded sentence: — Подяволите! — извика возникът. — Него все съга \n",
      "\n",
      "-\n",
      "Input sentence: —Итойсевърна,тавзеогнивото.Тогава\n",
      "Decoded sentence: — И той се върна, та внезговино. Това го ва \n",
      "\n",
      "-\n",
      "Input sentence: гоизтеглигореивойникътсенамериотновона\n",
      "Decoded sentence: го изтегли гореди от нчукстене за питоново на \n",
      "\n",
      "-\n",
      "Input sentence: Джобовете,раницата,чизмитеифуражкатамубяха\n",
      "Decoded sentence: Добовете, разцита, на 2 чиши и меркараха кащо бе на \n",
      "\n",
      "-\n",
      "Input sentence: —Каквощеправишстоваогниво?—попита\n",
      "Decoded sentence: — Какво ще правиш стова огниво? — попита \n",
      "\n",
      "-\n",
      "Input sentence: —Неетвояработа!—отвърнамагьосницата.—Ти\n",
      "Decoded sentence: — Не е твоя работа, — отвърна магьосницата. — И \n",
      "\n",
      "-\n",
      "Input sentence: —Ягледай!—речевойникът.—Кажизащотие\n",
      "Decoded sentence: — Я гледай! — рече от вникът. — Кажа — дощати е \n",
      "\n",
      "-\n",
      "Input sentence: —Нямадакажа!—извикамагьосницата.\n",
      "Decoded sentence: — Няма да кажа! — извика магьосницата.\n",
      "\n",
      "-\n",
      "Input sentence: Тогававойникътизвадисабятаиотсечеглаватай.\n",
      "Decoded sentence: Тогава своник корива се даби — тая отиче с главата. \n",
      "\n",
      "-\n",
      "Input sentence: вързавсичкитесипаривпрестилкатана\n",
      "Decoded sentence: върза всичките си пари в пристелката на \n",
      "\n",
      "-\n",
      "Input sentence: метнавързопанагърбаси,пъхнаогнивотовджоба\n",
      "Decoded sentence: мента в прода на пруба съси, по-хано видово в олад \n",
      "\n",
      "-\n",
      "Input sentence: Аградътбешевеликолепен.Войникътотидев\n",
      "Decoded sentence: А градът беше велило печер. Но сколките от два \n",
      "\n",
      "-\n",
      "Input sentence: странноприемница,поисканай-хубаватастаяи\n",
      "Decoded sentence: странни преницела и оскан на Хорчава а тата си \n",
      "\n",
      "-\n",
      "Input sentence: ястия,защотосегавечебешебогат.Слугата,\n",
      "Decoded sentence: ястия, защото сега вече беше горат. Сладката.\n",
      "\n",
      "-\n",
      "Input sentence: изчистичизмитему,навярнопомисли,четеса\n",
      "Decoded sentence: изчисти силемите, на беля по ослоци, честа се \n",
      "\n",
      "-\n",
      "Input sentence: вехтизатакъвбогатчовек.Нонадругияден\n",
      "Decoded sentence: вех — цая какво въпнато вечо. На други ден я \n",
      "\n",
      "-\n",
      "Input sentence: сикупииновичизми,ипрекраснидрехи.Той\n",
      "Decoded sentence: си купини и в кидини, и прекрасни дреб. Той и \n",
      "\n",
      "-\n",
      "Input sentence: важенгосподинизатовахоратамупоказваха\n",
      "Decoded sentence: важен господини на отхара от малка му бабава \n",
      "\n",
      "-\n",
      "Input sentence: забележителностинаграда.Разказахамуиза\n",
      "Decoded sentence: за белице летности на драба. Разказаха му и за \n",
      "\n",
      "-\n",
      "Input sentence: —Къдеможедаявидичовек?—попитавойникът.\n",
      "Decoded sentence: — Къде може да ядивид човек? — попита войникът.\n",
      "\n",
      "-\n",
      "Input sentence: —Никъде—казахамувсички.—Тяживеевголемия\n",
      "Decoded sentence: — Никъде — казаха му всички. — Тови веде ги в лиди \n",
      "\n",
      "-\n",
      "Input sentence: замък,заобиколеномножествокулиистени.При\n",
      "Decoded sentence: замък, забоки множено мновекът и билинсти. На 5 \n",
      "\n",
      "-\n",
      "Input sentence: отивасамоцарят,защотойепредсказано,чеще\n",
      "Decoded sentence: от и васам от рахя, а той ще поерес закода, не ме \n",
      "\n",
      "-\n",
      "Input sentence: „Искамиседаявидя“—помислисивойникът.Но\n",
      "Decoded sentence: „Искам си едяа видя — лози писи с водит. Ако бла \n",
      "\n",
      "-\n",
      "Input sentence: искапозволениезатова,разбирасе,неможешеда\n",
      "Decoded sentence: и са корговение на това, рабрази. Седне межеше да \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_text[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
