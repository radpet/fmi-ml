{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, StringIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_and_extract(url, path):\n",
    "#     request = requests.get(url)\n",
    "#     with ZipFile(BytesIO(request.content), \"r\") as file:\n",
    "#          file.extractall(path)\n",
    "\n",
    "# urls = [\"https://m2.chitanka.info/book/25-andersenovi-prikazki.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/2317-12-mita-v-bylgarskata-istorija.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6548-baba-djado-i-vnuche.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6761-vasilisa-prekrasna.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1824-atentati-koito-trjabvashe-da-promenjat-s.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/3103-601-izpitani-gotvarski-retsepti.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/4388-pet-moralni-eseta.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/366-ochertsi-za-prestypnija-svjat.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1031-hakeri-na-choveshkite-dushi.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6938-12-printsipa-na-proizvoditelnostta.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/5226-aleksandyr-makedonski.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6393-bogat-tatko-beden-tatko.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6748-edin-den-v-dreven-rim.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6006-vikingi-i-skrelingi-v-amerika.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1498-diktatorite.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/4098-izsledvane-na-istorijata.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1581-igrata-na-lisitsite.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1762-istorija-na-vtorata-svetovna-vojna.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/5084-razgromyt.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/8106-usmivka-v-polunosht.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1016-7-i-37-chudesa.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1453-razpoznavane-i-sybirane-na-bilki.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1475-grizhi-za-bebeto-i-deteto.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/524-az-i-moreto.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/257-vojnishka-tetradka.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/1435-biblija-star-zavet.txt.zip\",\n",
    "#        \"https://m2.chitanka.info/book/6709-david-i-goliat.txt.zip\"]\n",
    "\n",
    "            \n",
    "# for url in urls:\n",
    "#     download_and_extract(url,\"./training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    files = os.listdir(path)\n",
    "    lines = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(\"{}/{}\".format(path,file), encoding=\"utf-8-sig\") as file_content:\n",
    "            new_line = \"\"\n",
    "            for line in file_content:\n",
    "                line = line.strip()\n",
    "                if(len(new_line) + len(line) > max_line_len):\n",
    "                    tokens = line.split(\" \")\n",
    "                    \n",
    "                    for token in tokens:\n",
    "                        if(len(new_line) + len(token) < max_line_len):\n",
    "                            new_line +=token + \" \"\n",
    "                        else:\n",
    "                            lines.append(new_line)\n",
    "                            new_line = token\n",
    "                else:\n",
    "                    new_line +=line + \" \"\n",
    "                    \n",
    "                    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_corpus(\"./training/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335474"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['издателствона ЦК на ДКМС Държавна печатница ',\n",
       " '„ДимитърБлагоев“ София, 1983  Игор Всеволодович ',\n",
       " 'Можейко7 и 37 ЧУДЕС Главная редакция восточной ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [ line.replace(\" \", \"\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_CHAR = \"\\t\"\n",
    "END_CHAR = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = [ START_CHAR + line + END_CHAR for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tБазил Лидъл Харт История на Втората световна война \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['БазилЛидълХартИсториянаВторатасветовнавойна',\n",
       " 'ПредговорКогатоДезмъндФлауър,президентътна',\n",
       " 'издателство„Касъл“,мепомолиданапиша']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch = set()\n",
    "for line in input_text:\n",
    "    for c in line:\n",
    "        input_ch.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ch = list(sorted(input_ch))\n",
    "input_ch_len = len(input_ch)\n",
    "input_ch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ch = set()\n",
    "for line in target_text:\n",
    "    for c in line:\n",
    "        target_ch.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ch_len = len(target_ch)\n",
    "target_ch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch_idx = dict([(char,i) for i, char in enumerate(input_ch)])\n",
    "target_ch_idx = dict([(char,i) for i, char in enumerate(target_ch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "max_input_len = max([len(line) for line in input_text])\n",
    "max_target_len = max([len(line) for line in target_text])\n",
    "\n",
    "print(max_input_len)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_samples=[], target_samples=[]):\n",
    "    \n",
    "    if len(input_samples) != len(target_samples):\n",
    "        print(\"INPUT AND TARGET have diff size\")\n",
    "        return None\n",
    "    \n",
    "    max_input_len = max([len(sample) for sample in input_samples])\n",
    "    max_target_len = max([len(sample) for sample in target_samples])\n",
    "\n",
    "    # Тук [This index is for sample] [This index is for char in sample][This index is one hot encoding the char]\n",
    "    \n",
    "    encoder_input = np.zeros((len(input_samples), max_input_len, input_ch_len),dtype=\"float64\")\n",
    "    decoder_input = np.zeros((len(input_samples), max_target_len, target_ch_len),dtype=\"float64\")\n",
    "    decoder_output = np.zeros((len(input_samples), max_target_len, target_ch_len),dtype=\"float64\")\n",
    "    \n",
    "    \n",
    "    for i, (input_sample, target_sample) in enumerate(zip(input_samples, target_samples)):\n",
    "        for t, char in enumerate(input_sample):\n",
    "            encoder_input[i, t, input_ch_idx[char]] = 1.\n",
    "        for t, char in enumerate(target_sample):\n",
    "            decoder_input[i, t, target_ch_idx[char]] = 1.\n",
    "            if t > 0:\n",
    "                decoder_output[i, t - 1, target_ch_idx[char]] = 1.\n",
    "            \n",
    "    return encoder_input, decoder_input, decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SPACE_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, input_ch_len))\n",
    "encoder = LSTM(STATE_SPACE_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, target_ch_len))\n",
    "dropout = Dropout(0.2)(decoder_inputs)\n",
    "decoder_lstm = LSTM(STATE_SPACE_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dropout,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(target_ch_len, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 215)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None, 213)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 215)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 175104      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 128),  176128      dropout_1[0][0]                  \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 215)    27735       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 378,967\n",
      "Trainable params: 378,967\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 11s 470us/step - loss: 3.1263 - val_loss: 2.775432 - ETA: 0s - loss: 3.129\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 10s 428us/step - loss: 2.7407 - val_loss: 2.6888\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 10s 428us/step - loss: 2.6446 - val_loss: 2.5730TA: \n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 10s 428us/step - loss: 2.5381 - val_loss: 2.4654\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 10s 430us/step - loss: 2.4560 - val_loss: 2.3803\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 10s 432us/step - loss: 2.3956 - val_loss: 2.3166\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 10s 429us/step - loss: 2.3487 - val_loss: 2.2661\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 10s 431us/step - loss: 2.3191 - val_loss: 2.2297- ETA: 0s - loss: \n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 10s 433us/step - loss: 2.2906 - val_loss: 2.1984\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 10s 435us/step - loss: 2.2666 - val_loss: 2.1717\n",
      "30000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 16s 652us/step - loss: 1.7147 - val_loss: 1.5866\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 15s 638us/step - loss: 1.6730 - val_loss: 1.5632\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 15s 628us/step - loss: 1.6468 - val_loss: 1.5459\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 15s 618us/step - loss: 1.6286 - val_loss: 1.5333\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 15s 617us/step - loss: 1.6182 - val_loss: 1.5199\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 15s 617us/step - loss: 1.5983 - val_loss: 1.5074\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 15s 620us/step - loss: 1.5859 - val_loss: 1.4974\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 15s 616us/step - loss: 1.5745 - val_loss: 1.4874\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 15s 617us/step - loss: 1.5602 - val_loss: 1.4717\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 15s 616us/step - loss: 1.5487 - val_loss: 1.4699\n",
      "60000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 13s 535us/step - loss: 2.0232 - val_loss: 1.8878\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 11s 479us/step - loss: 1.9704 - val_loss: 1.8507\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 11s 467us/step - loss: 1.9401 - val_loss: 1.8192\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 11s 464us/step - loss: 1.9134 - val_loss: 1.7904\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 11s 469us/step - loss: 1.8885 - val_loss: 1.7651\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 11s 468us/step - loss: 1.8663 - val_loss: 1.7400\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 11s 464us/step - loss: 1.8466 - val_loss: 1.7181\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 11s 462us/step - loss: 1.8272 - val_loss: 1.6961\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 11s 469us/step - loss: 1.8091 - val_loss: 1.6756\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 11s 464us/step - loss: 1.7901 - val_loss: 1.6578\n",
      "90000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 71s 3ms/step - loss: 0.3969 - val_loss: 0.3732\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 60s 2ms/step - loss: 0.3891 - val_loss: 0.3701\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3861 - val_loss: 0.3678\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3838 - val_loss: 0.3661\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3819 - val_loss: 0.3651\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3802 - val_loss: 0.3632\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3788 - val_loss: 0.3623\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3783 - val_loss: 0.3621\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3766 - val_loss: 0.3598\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 59s 2ms/step - loss: 0.3748 - val_loss: 0.3582\n",
      "120000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 12s 517us/step - loss: 2.0973 - val_loss: 1.8927\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 11s 455us/step - loss: 1.9668 - val_loss: 1.8235\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 11s 455us/step - loss: 1.9194 - val_loss: 1.7799\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 11s 445us/step - loss: 1.8850 - val_loss: 1.7509\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.8556 - val_loss: 1.7187\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 10s 435us/step - loss: 1.8304 - val_loss: 1.6955\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.8090 - val_loss: 1.6738\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.7878 - val_loss: 1.6471\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 11s 438us/step - loss: 1.7625 - val_loss: 1.6224\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 11s 442us/step - loss: 1.7410 - val_loss: 1.6031\n",
      "150000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 11s 460us/step - loss: 1.7748 - val_loss: 1.5506\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.7421 - val_loss: 1.5323\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 10s 430us/step - loss: 1.7235 - val_loss: 1.5211\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.7068 - val_loss: 1.5010\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 11s 438us/step - loss: 1.6919 - val_loss: 1.4911\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 11s 442us/step - loss: 1.6783 - val_loss: 1.4779\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 10s 436us/step - loss: 1.6669 - val_loss: 1.4637\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.6560 - val_loss: 1.4505\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 10s 433us/step - loss: 1.6441 - val_loss: 1.4421\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 10s 436us/step - loss: 1.6322 - val_loss: 1.4306\n",
      "180000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 12s 507us/step - loss: 1.7662 - val_loss: 1.5953\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 12s 487us/step - loss: 1.7102 - val_loss: 1.5754\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 11s 465us/step - loss: 1.6892 - val_loss: 1.5361\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 11s 467us/step - loss: 1.6749 - val_loss: 1.5243\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 11s 465us/step - loss: 1.6618 - val_loss: 1.5013\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 11s 464us/step - loss: 1.6472 - val_loss: 1.4870\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 11s 468us/step - loss: 1.6400 - val_loss: 1.4756\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 11s 467us/step - loss: 1.6285 - val_loss: 1.4552\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 11s 460us/step - loss: 1.6205 - val_loss: 1.4547\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 11s 461us/step - loss: 1.6106 - val_loss: 1.4378\n",
      "210000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 11s 454us/step - loss: 1.7626 - val_loss: 1.5569\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 10s 435us/step - loss: 1.7280 - val_loss: 1.5389\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.7083 - val_loss: 1.5141\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.6928 - val_loss: 1.5047\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.6781 - val_loss: 1.4898\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.6657 - val_loss: 1.4737\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 11s 438us/step - loss: 1.6545 - val_loss: 1.4581\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 11s 438us/step - loss: 1.6436 - val_loss: 1.4585\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 11s 449us/step - loss: 1.6338 - val_loss: 1.4396\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 11s 455us/step - loss: 1.6228 - val_loss: 1.4306\n",
      "240000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 12s 482us/step - loss: 1.6371 - val_loss: 1.7799\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 10s 428us/step - loss: 1.6108 - val_loss: 1.7273 - loss: 1\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 10s 429us/step - loss: 1.5956 - val_loss: 1.6923\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 10s 433us/step - loss: 1.5809 - val_loss: 1.6752\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 10s 425us/step - loss: 1.5701 - val_loss: 1.6469\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 10s 430us/step - loss: 1.5580 - val_loss: 1.6304\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 10s 428us/step - loss: 1.5503 - val_loss: 1.6152\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 10s 429us/step - loss: 1.5418 - val_loss: 1.6107\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 10s 435us/step - loss: 1.5310 - val_loss: 1.5861\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 11s 447us/step - loss: 1.5197 - val_loss: 1.5673\n",
      "270000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 15s 606us/step - loss: 1.4586 - val_loss: 1.2156\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 13s 553us/step - loss: 1.3665 - val_loss: 1.1992\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 13s 542us/step - loss: 1.3383 - val_loss: 1.1817\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 13s 543us/step - loss: 1.3187 - val_loss: 1.1690\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 13s 545us/step - loss: 1.3008 - val_loss: 1.1727\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 13s 535us/step - loss: 1.2859 - val_loss: 1.1544\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 13s 542us/step - loss: 1.2752 - val_loss: 1.1514\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 13s 558us/step - loss: 1.2611 - val_loss: 1.1488\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 13s 554us/step - loss: 1.2541 - val_loss: 1.1369\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 13s 538us/step - loss: 1.2405 - val_loss: 1.1355\n",
      "300000\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 11s 452us/step - loss: 1.5591 - val_loss: 1.3218\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.4921 - val_loss: 1.2931\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 10s 429us/step - loss: 1.4715 - val_loss: 1.2866\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 10s 435us/step - loss: 1.4593 - val_loss: 1.2767\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 10s 436us/step - loss: 1.4461 - val_loss: 1.2665\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 11s 439us/step - loss: 1.4357 - val_loss: 1.2703\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 10s 431us/step - loss: 1.4270 - val_loss: 1.2606\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 10s 430us/step - loss: 1.4160 - val_loss: 1.2509\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 10s 430us/step - loss: 1.4100 - val_loss: 1.2398\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 10s 437us/step - loss: 1.4040 - val_loss: 1.2351\n",
      "330000\n",
      "Train on 4379 samples, validate on 1095 samples\n",
      "Epoch 1/10\n",
      "4379/4379 [==============================] - 2s 440us/step - loss: 1.4457 - val_loss: 1.2227\n",
      "Epoch 2/10\n",
      "4379/4379 [==============================] - 2s 440us/step - loss: 1.4306 - val_loss: 1.2093\n",
      "Epoch 3/10\n",
      "4379/4379 [==============================] - 2s 439us/step - loss: 1.4192 - val_loss: 1.2083\n",
      "Epoch 4/10\n",
      "4379/4379 [==============================] - 2s 437us/step - loss: 1.4123 - val_loss: 1.2220\n",
      "Epoch 5/10\n",
      "4379/4379 [==============================] - 2s 440us/step - loss: 1.4105 - val_loss: 1.2014\n",
      "Epoch 6/10\n",
      "4379/4379 [==============================] - 2s 445us/step - loss: 1.3974 - val_loss: 1.1962\n",
      "Epoch 7/10\n",
      "4379/4379 [==============================] - 2s 440us/step - loss: 1.3947 - val_loss: 1.2028\n",
      "Epoch 8/10\n",
      "4379/4379 [==============================] - 2s 440us/step - loss: 1.3914 - val_loss: 1.1932\n",
      "Epoch 9/10\n",
      "4379/4379 [==============================] - 2s 441us/step - loss: 1.3860 - val_loss: 1.2077\n",
      "Epoch 10/10\n",
      "4379/4379 [==============================] - 2s 442us/step - loss: 1.3879 - val_loss: 1.2082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7a868ab048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30000\n",
    "cursor = 0\n",
    "while cursor+batch_size < len(input_text):\n",
    "    gc.collect()\n",
    "    encoder_input_data, decoder_input_data, decoder_target_data = encode(input_samples=input_text[cursor:cursor+batch_size],\n",
    "                                                                    target_samples=target_text[cursor:cursor+batch_size])\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)\n",
    "    cursor+=batch_size\n",
    "    print(cursor)\n",
    "    del encoder_input_data,decoder_input_data, decoder_target_data\n",
    "    gc.collect()\n",
    "    \n",
    "gc.collect()\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = encode(input_samples=input_text[cursor:],\n",
    "                                                                    target_samples=target_text[cursor:])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save(\"more_more_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(STATE_SPACE_DIM,))\n",
    "decoder_state_input_c = Input(shape=(STATE_SPACE_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_ch_idx = dict(\n",
    "    (i, char) for char, i in input_ch_idx.items())\n",
    "reverse_target_ch_idx= dict(\n",
    "    (i, char) for char, i in target_ch_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  \n",
    "    target_seq = np.zeros((1, 1, len(target_ch)))\n",
    "   \n",
    "    target_seq[0, 0, target_ch_idx[START_CHAR]] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_ch_idx[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        \n",
    "        if (sampled_char == END_CHAR or\n",
    "           len(decoded_sentence) > max_target_len):\n",
    "            stop_condition = True\n",
    "\n",
    "       \n",
    "        target_seq = np.zeros((1, 1, len(target_ch)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: БазилЛидълХартИсториянаВторатасветовнавойна\n",
      "Decoded sentence: ченатой де и полиго — кура си града, както се да \n",
      "\n",
      "-\n",
      "Input sentence: ПредговорКогатоДезмъндФлауър,президентътна\n",
      "Decoded sentence: указазарава на на прединстта на строите на \n",
      "\n",
      "-\n",
      "Input sentence: издателство„Касъл“,мепомолиданапиша\n",
      "Decoded sentence: Призсъв на граната и се на върха и да се извечен \n",
      "\n",
      "-\n",
      "Input sentence: предговоракъмкнигатанасъпругами„Историяна\n",
      "Decoded sentence: историята са да се нагуда — в Борма на Бура се са \n",
      "\n",
      "-\n",
      "Input sentence: Вторатасветовнавойна“,многоскоросидадох\n",
      "Decoded sentence: хълмалкоредвели на голата с проди. И да се \n",
      "\n",
      "-\n",
      "Input sentence: сметка,чезадаблагодарянавсички,които\n",
      "Decoded sentence: селитавали въще ст полали на била преди са на \n",
      "\n",
      "-\n",
      "Input sentence: помогналивподготовкатапонаписванетой,трябва\n",
      "Decoded sentence: редитекатели на преди. Простава пред Ангура. И да \n",
      "\n",
      "-\n",
      "Input sentence: даотдамзаслуженопризнаниенастотицихора,от\n",
      "Decoded sentence: Ванрайна.Кога обил бил да ставал на строята с мо \n",
      "\n",
      "-\n",
      "Input sentence: фелдмаршалидоредници,професори,студентии\n",
      "Decoded sentence: хлама.Бару бил до издили на да се изкали на се и \n",
      "\n",
      "-\n",
      "Input sentence: приятели,скоитоБазилеподдържалвръзкипрез\n",
      "Decoded sentence: видянима не и в пред вреден на пред Понтара. Но \n",
      "\n",
      "-\n",
      "Input sentence: активнатасиизследователскадейност,Впредговора\n",
      "Decoded sentence: семъжила за е в съм работа са правята храма на \n",
      "\n",
      "-\n",
      "Input sentence: нанеговите„Мемоари“тойпише,че„внай-добрия\n",
      "Decoded sentence: седизиговасостине на се на времена на Анфина. На \n",
      "\n",
      "-\n",
      "Input sentence: случаймемоаритесаархивинаприятелствата,ав\n",
      "Decoded sentence: жилтите кала да да го мало по-най-мата страна му \n",
      "\n",
      "-\n",
      "Input sentence: товаотношениеазсъмбилмногооблагодетелстван“.\n",
      "Decoded sentence: започвализа пробъта мога в свениците на се и \n",
      "\n",
      "-\n",
      "Input sentence: Тази„История“същоебилаоблагодетелстванаот\n",
      "Decoded sentence: обръща,би пърго в мъста. В сегата се си и да \n",
      "\n",
      "-\n",
      "Input sentence: подобниприятелства.ОтмалъкБазилебил\n",
      "Decoded sentence: койтосе зачина и да когато издигал на Била \n",
      "\n",
      "-\n",
      "Input sentence: пристрастенкъмигритеитактикатанаигритеи\n",
      "Decoded sentence: смърта.Та го в дашени и краго се се отради на доли \n",
      "\n",
      "-\n",
      "Input sentence: катоученикесъбиралвсякаквисведенияи\n",
      "Decoded sentence: прадедитеси измал в пред страните на повен — \n",
      "\n",
      "-\n",
      "Input sentence: вестникарскиизрезкизатях,напримерзапървите\n",
      "Decoded sentence: чормовеникако се постова от съвета. И да си — кал \n",
      "\n",
      "-\n",
      "Input sentence: днинаавиацията,когатолетцитесабилинегови\n",
      "Decoded sentence: Брадатапуднава не е от койно се на колко на край — \n",
      "\n",
      "-\n",
      "Input sentence: кумири.Тойзапазитозинавикпрезцелиясиживот\n",
      "Decoded sentence: пълъхахърмал с браго са храмата си само на \n",
      "\n",
      "-\n",
      "Input sentence: завсичкитесинепрекъснаторазширявяващисе\n",
      "Decoded sentence: скалата цертница. И да си казари простира, които \n",
      "\n",
      "-\n",
      "Input sentence: интереси.Порадитоваследсмърттасиостави\n",
      "Decoded sentence: Ванорана, само и страли правил само и не изкали и \n",
      "\n",
      "-\n",
      "Input sentence: стотицихилядиизрезки,писма,бележки,памфлетаи\n",
      "Decoded sentence: целатанита. Синалия се се надили в каката на \n",
      "\n",
      "-\n",
      "Input sentence: другиподобнипотеми,коитовариратотводенето\n",
      "Decoded sentence: страбала.Пред няколо мистия на наричал и се на се \n",
      "\n",
      "-\n",
      "Input sentence: набронетанковатавойнадомодитевоблеклото.\n",
      "Decoded sentence: преднастукряваният на страните на малки и калия \n",
      "\n",
      "-\n",
      "Input sentence: По-късноподформатанадневник,иликактотойго\n",
      "Decoded sentence: напокърхората. Не продълга с да се подили в \n",
      "\n",
      "-\n",
      "Input sentence: наричаше„Разговорнизаписки“,записваше\n",
      "Decoded sentence: койтои за плази върху предини с на се от дали \n",
      "\n",
      "-\n",
      "Input sentence: разговорите,коитобеводилпотеми,\n",
      "Decoded sentence: пеликороскострои се правила за само изкажени с са \n",
      "\n",
      "-\n",
      "Input sentence: представляващизанегоособенинтерес,ито\n",
      "Decoded sentence: име. Тай-отмули прави са в свенали на Анфирания и \n",
      "\n",
      "-\n",
      "Input sentence: възможнонай-скороследтова.Първатамукнига,\n",
      "Decoded sentence: сенавълхуле в драгато на света на по-маторите. \n",
      "\n",
      "-\n",
      "Input sentence: издаденаследвойната—„Другатастрананахълма“\n",
      "Decoded sentence: вярямене черали на буда са на храма. И да и кули \n",
      "\n",
      "-\n",
      "Input sentence: (TheOtherSideoftheHill),съдържаше\n",
      "Decoded sentence: наносо призи за имали стрини и на иния и Ангия, \n",
      "\n",
      "-\n",
      "Input sentence: разговоритемусгерманскигенерали,държаникато\n",
      "Decoded sentence: слещноте,каподна да правото съм стората с тол \n",
      "\n",
      "-\n",
      "Input sentence: военнопленницивАнглия.Многооттяхбяхачели\n",
      "Decoded sentence: Събрабите да глада правят му да се \n",
      "\n",
      "-\n",
      "Input sentence: книгитемуотпредивойнатаисготовностобсъждаха\n",
      "Decoded sentence: „изголеветали на Алекира се изличали и да се \n",
      "\n",
      "-\n",
      "Input sentence: снеговоеннитекампании,коитобяхаводили.През\n",
      "Decoded sentence: споделят,но с купувала на града. На намала на \n",
      "\n",
      "-\n",
      "Input sentence: декември1963г.,връщайкисеназадкъмонова\n",
      "Decoded sentence: гобетопъща в запавала в карата на страна. На се \n",
      "\n",
      "-\n",
      "Input sentence: време,тойпише:„Бележказатова,защоикак\n",
      "Decoded sentence: несе камо за стрилите стори и се изкличили с \n",
      "\n",
      "-\n",
      "Input sentence: написахтазикнига“,вкоятообясняваващотези\n",
      "Decoded sentence: просростра предите. Претора. Това съде се от \n",
      "\n",
      "-\n",
      "Input sentence: запискисатолковаценнизанего:„Когатопрез\n",
      "Decoded sentence: властите,на място от поление на Ангора, който се \n",
      "\n",
      "-\n",
      "Input sentence: 20-теи30-тегодиниизучавахсъбитията,отнасящи\n",
      "Decoded sentence: дана до изличат и церите си с на Ангура и синия \n",
      "\n",
      "-\n",
      "Input sentence: седоПърватасветовнавойна,разбрахкакваголяма\n",
      "Decoded sentence: убедина е постровала на Акоро — се изкалявал пости \n",
      "\n",
      "-\n",
      "Input sentence: частотисториятаоставанеизясненанапълно,тъй\n",
      "Decoded sentence: екопедиция,който простъпат камал на се изкала на \n",
      "\n",
      "-\n",
      "Input sentence: катонееималобезпристрастениподготвенпо\n",
      "Decoded sentence: додаща не за порхава в дълби с весет в народи. \n",
      "\n",
      "-\n",
      "Input sentence: историяизследовател,койтодапотвърдиида\n",
      "Decoded sentence: къщнособробвали неподновете на народина на \n",
      "\n",
      "-\n",
      "Input sentence: запишеонова,коетовоеначалницитедействителноса\n",
      "Decoded sentence: родоватав Броита кака. Но се страли възмата на \n",
      "\n",
      "-\n",
      "Input sentence: мислелинавремето,задаслужикатокорективна\n",
      "Decoded sentence: дестили в дамя и на се изминало не се отворяли на \n",
      "\n",
      "-\n",
      "Input sentence: по-къснитеимспомени.Започнадаставаочевидно,\n",
      "Decoded sentence: запо-редениятана от се от черия с нарочите на \n",
      "\n",
      "-\n",
      "Input sentence: честечениенавреметочестоспоменитена\n",
      "Decoded sentence: хралеопоганова не се от приличани с страни \n",
      "\n",
      "-\n",
      "Input sentence: участницитевдраматичнитесъбитиядопълнителносе\n",
      "Decoded sentence: оселцата кара и по-кори с престиранията на Антор \n",
      "\n",
      "-\n",
      "Input sentence: оцветяватиизопачават,асгодинитетази\n",
      "Decoded sentence: пимиково на доли за погава са са пазила на най-\n",
      "\n",
      "-\n",
      "Input sentence: тенденциясезасилва.Ощеповечечеофициалните\n",
      "Decoded sentence: кракатата не свелите с било страните си и страни \n",
      "\n",
      "-\n",
      "Input sentence: документичестопътинеотразяваттехнитеистински\n",
      "Decoded sentence: гранин,постъващия на моле да се изкали на Ангура, \n",
      "\n",
      "-\n",
      "Input sentence: разбиранияицели,апонякогадорисасъставени\n",
      "Decoded sentence: околобули за изливали с място. На се само \n",
      "\n",
      "-\n",
      "Input sentence: така,чедагиприкрият.Затова,когатопосещавах\n",
      "Decoded sentence: поробениятана Буда в под върха на се на кулата — \n",
      "\n",
      "-\n",
      "Input sentence: британскиисъюзническивоеначалници,сиводех\n",
      "Decoded sentence: наи арисината и не бъде бъда подади на малки \n",
      "\n",
      "-\n",
      "Input sentence: изчерпателни«историческибележки»заразговорите\n",
      "Decoded sentence: отказванекър по се при 1941 гадана на се \n",
      "\n",
      "-\n",
      "Input sentence: стях,катоособеностарателносизаписвах\n",
      "Decoded sentence: предоложениятезна веди за прави на Азуда и да \n",
      "\n",
      "-\n",
      "Input sentence: изразенитеоттяхмнения,коитослужехаза\n",
      "Decoded sentence: Разбирасе. Народал стогат се на селите на войни \n",
      "\n",
      "-\n",
      "Input sentence: допълненадокументалнитеархиви,асъщоикато\n",
      "Decoded sentence: неговеден,со на се е на неговените стройно с \n",
      "\n",
      "-\n",
      "Input sentence: корективнамемоаритеиразказитеим,писани\n",
      "Decoded sentence: годаменима се не страни с кравите на Анфина — изи \n",
      "\n",
      "-\n",
      "Input sentence: годинипо-късно.Вкраянавойнатаимахвъзможност\n",
      "Decoded sentence: иигоитевията, се сметни, колко се изличани \n",
      "\n",
      "-\n",
      "Input sentence: даразпитвамгерманскикомандири,държаникато\n",
      "Decoded sentence: по-мудите водименда се храме от Бодин само на се и \n",
      "\n",
      "-\n",
      "Input sentence: военнопленници,идаводястяхдългиразговори\n",
      "Decoded sentence: същетевноса каза за нароза на се извен \n",
      "\n",
      "-\n",
      "Input sentence: кактозаоперациите,вкоитобяхаучаствали,така\n",
      "Decoded sentence: по-калулиците,който през правите са си и кала на \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ипопо-широккръгвъпроси.Естествено,тезимои\n",
      "Decoded sentence: чеамасъщи да деле и да страната към това. Това се \n",
      "\n",
      "-\n",
      "Input sentence: запискинебихамоглидаосветлятнапълно\n",
      "Decoded sentence: Катоба от сирови и подарва в краните на синия \n",
      "\n",
      "-\n",
      "Input sentence: достовернотова,коетотесамислелипрединякои\n",
      "Decoded sentence: познавана нариза. По толко се съде какво си \n",
      "\n",
      "-\n",
      "Input sentence: конкретнисъбитияирешения,новсепакса\n",
      "Decoded sentence: никоикара и и са послените на страната на сери \n",
      "\n",
      "-\n",
      "Input sentence: направенипредиизлизанетонамемоарите,замъглени\n",
      "Decoded sentence: сдрагости пъте. Ази по пред 1941 годана на Акара, \n",
      "\n",
      "-\n",
      "Input sentence: доизвестнастепенотизминалитегодини,но\n",
      "Decoded sentence: наначастници за нашили се на съветата страните \n",
      "\n",
      "-\n",
      "Input sentence: технитеразказимогатдабъдатсъпоставенис\n",
      "Decoded sentence: пробелиза запазвани на се изглавал с \n",
      "\n",
      "-\n",
      "Input sentence: разказитенадругисвидетелинасъщитесъбития,\n",
      "Decoded sentence: комнада повени на волите на Буд пред Бермани и \n",
      "\n",
      "-\n",
      "Input sentence: кактоисдокументалнитеархиви“.Читателитена\n",
      "Decoded sentence: Мекайкразвочиния — в кази се възножели се изполече \n",
      "\n",
      "-\n",
      "Input sentence: тази„История“щевидятотбележкитеподлинияза\n",
      "Decoded sentence: зачуджосе изведини пред пред на какал сами си и \n",
      "\n",
      "-\n",
      "Input sentence: тезиразговорикактесаустоялинаизпитаниетона\n",
      "Decoded sentence: маполасели се зличени. По се подобил не предински \n",
      "\n",
      "-\n",
      "Input sentence: изминалотовремеинапродължилитепрезгодините\n",
      "Decoded sentence: крепотттасе по трони на врава. На него и при и \n",
      "\n",
      "-\n",
      "Input sentence: повторнипроверкиотстрананаБазил.Вначалото\n",
      "Decoded sentence: наме,кържили да се още пространи в дрежните \n",
      "\n",
      "-\n",
      "Input sentence: на1946г.бригаденкомандирнакралскиятанков\n",
      "Decoded sentence: подобно.То чожи и и синики сторите и и на и \n",
      "\n",
      "-\n",
      "Input sentence: полкпомолиБазилданапишеисториянаполкаина\n",
      "Decoded sentence: Анлуи,нойто в дове да се наричани — на \n",
      "\n",
      "-\n",
      "Input sentence: неговитепредшественици,катообхванедвете\n",
      "Decoded sentence: посрещнавилизвения на войно на Акрай се от \n",
      "\n",
      "-\n",
      "Input sentence: световнивойниигодинитемеждутях.Товабеше\n",
      "Decoded sentence: вреджавоси му с гора, правили на Ангорите и в \n",
      "\n",
      "-\n",
      "Input sentence: огромназадача,коятоотнемногогодини,икнигата\n",
      "Decoded sentence: година е на били жесто и да пред нагодани \n",
      "\n",
      "-\n",
      "Input sentence: неможадабъдепубликуванаот„Касъл“до1958г.\n",
      "Decoded sentence: ексересиния и друго престора. Той се съставал — \n",
      "\n",
      "-\n",
      "Input sentence: Обачепроучването,коетотрябвашедасенаправиза\n",
      "Decoded sentence: основана изседена на най-сторите съм страйка на Буд \n",
      "\n",
      "-\n",
      "Input sentence: „Танковете“(Tanks),мупомогнамного,когато\n",
      "Decoded sentence: станалата количаски с крата на Бура. На \n",
      "\n",
      "-\n",
      "Input sentence: Базилзапочнадапишетази„История“,защототой\n",
      "Decoded sentence: Замблавидайстко — не до подал в Бодин — той на Ако \n",
      "\n",
      "-\n",
      "Input sentence: бешеустановилличнипознанствасмногоот\n",
      "Decoded sentence: определилисърото на се изпорачени в кратата на се \n",
      "\n",
      "-\n",
      "Input sentence: по-младитекомандири,коитосебяхасражавалииот\n",
      "Decoded sentence: антрустеята храдата в отрани на Бодийски се отрали \n",
      "\n",
      "-\n",
      "Input sentence: дветестрани,катосъщевременнобешеводилмного\n",
      "Decoded sentence: използованита на продължани поднаси и да \n",
      "\n",
      "-\n",
      "Input sentence: продължителниразговористакивастарииценни\n",
      "Decoded sentence: имонова,иче и препървето с стругите си и да \n",
      "\n",
      "-\n",
      "Input sentence: приятели,катофелдмаршалМонтгомъри,фелдмаршал\n",
      "Decoded sentence: датарито. Проба — за сигури с били са на се \n",
      "\n",
      "-\n",
      "Input sentence: Александър,фелдмаршалОкинлек,атакасъщос\n",
      "Decoded sentence: изостванепризили пели на продоби. По-мало пред И \n",
      "\n",
      "-\n",
      "Input sentence: неговитетанкистиисмногогерманскигенерали„от\n",
      "Decoded sentence: товане означана за предежение на времена. На най-\n",
      "\n",
      "-\n",
      "Input sentence: другатастрананахълма“.СледВойнатаза\n",
      "Decoded sentence: КВирна.И тига и палици и храмал и да изкули и да \n",
      "\n",
      "-\n",
      "Input sentence: независимостпрез1946г.вИзраелдойдоха\n",
      "Decoded sentence: деесо олини — в Аразка на година — на която \n",
      "\n",
      "-\n",
      "Input sentence: израелскиофицериотнай-различниранговедасе\n",
      "Decoded sentence: Рамбладащи, която весен пред със отростите. В \n",
      "\n",
      "-\n",
      "Input sentence: консултиратсБазилзасформиранетонатяхната\n",
      "Decoded sentence: труда,за за пред кази се сиците на Анцирания — \n",
      "\n",
      "-\n",
      "Input sentence: армия.СредтяхбешеИгалАлон,скоготостанахме\n",
      "Decoded sentence: другилобаго дредина Сили — какво пред Ака се \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_text[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_name):\n",
    "    encoded = np.zeros (( 1 , max_input_len, input_ch_len ),dtype=\"float64\" )\n",
    "    for t, char in enumerate(input_name):\n",
    "        encoded[0, t, input_ch_idx[char]] = 1.\n",
    "    return encoded[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "саййше не жило пакула за Мал — \n",
      "\n",
      "байййногожели от ула, и пазу — \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode_sequence(encode(\"дасмеживииздрави\")))\n",
    "print(decode_sequence(encode(\"замногогодини\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
